This project presents a approach to sign language translation by integrating sentiment analysis to recognize both hand gestures and facial expressions. Unlike conventional systems that focus solely on hand movements, our system aims to bridge the communication gap for deaf and hard-of-hearing individuals by capturing the emotional tone conveyed through facial expressions. Sentiment analysis enhances the depth of translation by interpreting emotions, ensuring more accurate and nuanced communication. The project employs deep learning models for hand gesture recognition and facial emotion detection, thus providing a more holistic and context-aware sign language translation.
